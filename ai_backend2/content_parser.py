"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç–µ–π –ø–æ URL
"""

import requests
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import re
from content_cleaner import ContentCleaner
from text_chunker import TextChunker, TextChunk


class ArticleContent:
    def __init__(self, url: str, title: str, content: str, meta_description: str = "", 
                 cleaned_content: str = "", content_stats: dict = None, 
                 chunks: List[TextChunk] = None, chunking_stats: dict = None):
        self.url = url
        self.title = title
        self.content = content
        self.meta_description = meta_description
        self.cleaned_content = cleaned_content or content
        self.word_count = len(content.split()) if content else 0
        self.cleaned_word_count = len(cleaned_content.split()) if cleaned_content else 0
        self.content_stats = content_stats or {}
        self.chunks = chunks or []
        self.chunking_stats = chunking_stats or {}
        
    def to_dict(self) -> Dict:
        return {
            "url": self.url,
            "title": self.title,
            "content": self.content,
            "cleaned_content": self.cleaned_content,
            "meta_description": self.meta_description,
            "word_count": self.word_count,
            "cleaned_word_count": self.cleaned_word_count,
            "content_stats": self.content_stats,
            "chunks": [chunk.to_dict() for chunk in self.chunks],
            "chunking_stats": self.chunking_stats
        }


class ContentParser:
    def __init__(self, enable_cleaning: bool = True, enable_chunking: bool = False, 
                 chunk_size: int = 1000, cleaning_settings: dict = None):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        self.timeout = 20
        self.enable_cleaning = enable_cleaning
        self.enable_chunking = enable_chunking
        self.cleaning_settings = cleaning_settings or {}
        self.cleaner = ContentCleaner(self.cleaning_settings) if enable_cleaning else None
        self.chunker = TextChunker(target_chunk_size=chunk_size) if enable_chunking else None
        
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if not text:
            return ""
            
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # –£–¥–∞–ª—è–µ–º HTML entities
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&quot;', '"')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&amp;', '&')
        
        return text.strip()
    
    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ HTML"""
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 
                        'aside', 'advertisement', 'ads', 'sidebar']):
            tag.decompose()
            
        # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ ID (—Ç–∏–ø–∏—á–Ω—ã–π –º—É—Å–æ—Ä)
        unwanted_selectors = [
            '[class*="nav"]', '[class*="menu"]', '[class*="header"]', 
            '[class*="footer"]', '[class*="sidebar"]', '[class*="widget"]',
            '[class*="ad"]', '[class*="banner"]', '[class*="popup"]',
            '[id*="nav"]', '[id*="menu"]', '[id*="header"]', 
            '[id*="footer"]', '[id*="sidebar"]'
        ]
        
        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        content_selectors = [
            'article', '[role="main"]', 'main', '.content', '.post-content',
            '.entry-content', '.article-content', '.post-body', '.text',
            '.post', '.blog-post', '.article', '.story', '.news-content',
            '.page-content', '.main-content', '.content-area', '.entry',
            '.post-text', '.article-text', '.content-text', '.text-content',
            '#content', '#main', '.main-content'
        ]
        
        main_content = None
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # –í—ã–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
                main_content = max(elements, key=lambda x: len(x.get_text()))
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—â–µ–º –ø–æ —Ç–µ–≥–∞–º
        if not main_content:
            for tag in ['article', 'main', 'div']:
                elements = soup.find_all(tag)
                if elements:
                    # –ë–µ—Ä–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
                    main_content = max(elements, key=lambda x: len(x.get_text()))
                    break
        
        if not main_content:
            main_content = soup
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        content_parts = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: p, div, span —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
        for tag in main_content.find_all(['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
            text = tag.get_text().strip()
            if len(text) > 20:  # –£–º–µ–Ω—å—à–∏–ª–∏ –º–∏–Ω–∏–º—É–º –¥–æ 20 —Å–∏–º–≤–æ–ª–æ–≤
                content_parts.append(text)
        
        # –ï—Å–ª–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        if len(' '.join(content_parts)) < 150:
            content_parts = [main_content.get_text()]
            
        full_content = '\n\n'.join(content_parts)
        return self._clean_text(full_content)
    
    def parse_article(self, url: str) -> Optional[ArticleContent]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
        try:
            print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º title
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º meta description
            meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
            meta_description = ""
            if meta_desc_tag and meta_desc_tag.get('content'):
                meta_description = meta_desc_tag['content'].strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_main_content(soup, url)
            
            if len(content) < 50:
                print(f"‚ö†Ô∏è –ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤): {url}")
                return None
            
            # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            cleaned_content = content
            content_stats = {}
            
            if self.enable_cleaning and self.cleaner:
                cleaned_content = self.cleaner.clean_content(content)
                content_stats = self.cleaner.get_content_stats(cleaned_content)
                
                if len(cleaned_content) < 50:
                    print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({len(cleaned_content)} —Å–∏–º–≤–æ–ª–æ–≤): {url}")
                    return None
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            chunks = []
            chunking_stats = {}
            
            if self.enable_chunking and self.chunker:
                text_for_chunking = cleaned_content if cleaned_content else content
                chunks = self.chunker.chunk_text(text_for_chunking)
                chunking_stats = self.chunker.get_chunking_stats(chunks)
                
            article = ArticleContent(url, title, content, meta_description, 
                                   cleaned_content, content_stats, chunks, chunking_stats)
            
            chunk_info = f" (—á–∞–Ω–∫–æ–≤: {len(chunks)})" if chunks else ""
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {article.word_count} —Å–ª–æ–≤ (–æ—á–∏—â–µ–Ω–æ: {article.cleaned_word_count} —Å–ª–æ–≤){chunk_info}")
            
            return article
            
        except requests.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {str(e)}")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {str(e)}")
            return None
    
    def parse_multiple_articles(self, urls: List[str], delay: float = 1.0) -> List[ArticleContent]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        articles = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ URL...")
            
            article = self.parse_article(url)
            if article:
                articles.append(article)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(urls):
                time.sleep(delay)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç: —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(articles)} –∏–∑ {len(urls)} —Å—Ç–∞—Ç–µ–π")
        return articles