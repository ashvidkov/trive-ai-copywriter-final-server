"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ—á–∏—Å—Ç–∫–∏ —Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import re
from typing import List, Set
from collections import Counter


class ContentCleaner:
    def __init__(self, settings=None):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—á–∏—Å—Ç–∫–∏
        self.settings = settings or {}
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        self.navigation_keywords = {
            '–≥–ª–∞–≤–Ω–∞—è', '–º–µ–Ω—é', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø–æ–∏—Å–∫',
            '–∫–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞', '–æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '—Ä–µ–∫–ª–∞–º–∞',
            '–ø–æ–¥–ø–∏—Å–∫–∞', '–∞—Ä—Ö–∏–≤', '—Ä—É–±—Ä–∏–∫–∏', '—Ç–µ–≥–∏', '–º–µ—Ç–∫–∏', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ',
            '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è', '–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ', 'facebook', 'twitter',
            '–æ–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏', 'telegram', 'whatsapp', 'viber', 'instagram'
        }
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        self.technical_phrases = {
            '–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã', '–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤', '–ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤',
            '—Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è —Å–∞–π—Ç–∞', '—Ä–µ–¥–∞–∫—Ü–∏—è –Ω–µ –Ω–µ—Å–µ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏',
            '–º–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤', 'javascript', 'cookie', '–±—Ä–∞—É–∑–µ—Ä', 'adobe flash',
            'internet explorer', 'chrome', 'firefox', 'safari', '–≤–µ—Ä—Å–∏—è –¥–ª—è –ø–µ—á–∞—Ç–∏'
        }
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
        self.min_paragraph_length = self.settings.get('min_paragraph_length', 50)
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
        self.max_paragraph_length = self.settings.get('max_paragraph_length', 2000)
        
    def _remove_technical_blocks(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏"""
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line_lower = line.lower().strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not line_lower:
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            if any(keyword in line_lower for keyword in self.navigation_keywords):
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã
            if any(phrase in line_lower for phrase in self.technical_phrases):
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–æ–ª—å–∫–æ —Å —Ü–∏—Ñ—Ä–∞–º–∏/–¥–∞—Ç–∞–º–∏/–∑–Ω–∞–∫–∞–º–∏
            if re.match(r'^[\d\s\.\-\:\,\/]+$', line_lower):
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–º–µ–Ω–µ–µ 15 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(line_lower) < 15:
                continue
                
            clean_lines.append(line.strip())
        
        return '\n'.join(clean_lines)
    
    def _remove_duplicates(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –∞–±–∑–∞—Ü–µ–≤"""
        paragraphs = text.split('\n\n')
        unique_paragraphs = []
        seen_paragraphs = set()
        
        for paragraph in paragraphs:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized = re.sub(r'\s+', ' ', paragraph.lower().strip())
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            if len(normalized) < self.min_paragraph_length:
                continue
            if len(paragraph) > self.max_paragraph_length:
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentences = re.split(r'[.!?]+', paragraph)
                for sentence in sentences:
                    if self.min_paragraph_length <= len(sentence.strip()) <= 500:
                        sentence_norm = re.sub(r'\s+', ' ', sentence.lower().strip())
                        if sentence_norm not in seen_paragraphs:
                            unique_paragraphs.append(sentence.strip() + '.')
                            seen_paragraphs.add(sentence_norm)
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
            if normalized not in seen_paragraphs:
                unique_paragraphs.append(paragraph.strip())
                seen_paragraphs.add(normalized)
        
        return '\n\n'.join(unique_paragraphs)
    
    def _filter_relevant_content(self, text: str) -> str:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        paragraphs = text.split('\n\n')
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ
        scored_paragraphs = []
        
        for paragraph in paragraphs:
            if len(paragraph.strip()) < self.min_paragraph_length:
                continue
                
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å" –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            score = 0
            
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
            words = re.findall(r'\b[–∞-—è—ë]+\b', paragraph.lower())
            if len(words) > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤
                score += len(words) * 0.1
                
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            numbers = re.findall(r'\d+', paragraph)
            score += len(numbers) * 2
            
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            technical_words = re.findall(r'\b(–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ|—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è|–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ|–ø—Ä–æ—Ü–µ—Å—Å|–º–µ—Ç–æ–¥|—Å–∏—Å—Ç–µ–º–∞|–º–∞—Ç–µ—Ä–∏–∞–ª|–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ)\b', paragraph.lower())
            score += len(technical_words) * 3
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
            word_counts = Counter(words)
            repeated_words = sum(1 for count in word_counts.values() if count > 3)
            score -= repeated_words * 2
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –º–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            if len(words) < 20:
                score -= 5
                
            scored_paragraphs.append((score, paragraph))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º –ª—É—á—à–∏–µ
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # –ë–µ—Ä–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_threshold = self.settings.get('relevance_threshold', 0.7)
        take_count = min(len(scored_paragraphs), max(int(len(scored_paragraphs) * relevance_threshold), 15))
        best_paragraphs = [p[1] for p in scored_paragraphs[:take_count]]
        
        return '\n\n'.join(best_paragraphs)
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n[‚Ä¢\-\*\+]\s*', '\n', text)
        
        # –£–±–∏—Ä–∞–µ–º HTML entities
        html_entities = {
            '&nbsp;': ' ', '&quot;': '"', '&lt;': '<', '&gt;': '>',
            '&amp;': '&', '&copy;': '¬©', '&reg;': '¬Æ', '&trade;': '‚Ñ¢',
            '&laquo;': '¬´', '&raquo;': '¬ª', '&mdash;': '‚Äî', '&ndash;': '‚Äì'
        }
        for entity, replacement in html_entities.items():
            text = text.replace(entity, replacement)
        
        return text.strip()
    
    def clean_content(self, content: str) -> str:
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content or len(content) < 100:
            return content
            
        print("üßπ –ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –æ—á–∏—Å—Ç–∫–∏
        mode = self.settings.get('mode', 'automatic')
        
        if mode == 'manual':
            # –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if self.settings.get('remove_technical_blocks', True):
                content = self._remove_technical_blocks(content)
                print(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if self.settings.get('remove_duplicates', True):
                content = self._remove_duplicates(content)
                print(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if self.settings.get('filter_relevance', True):
                content = self._filter_relevant_content(content)
                print(f"üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è)
            content = self._remove_technical_blocks(content)
            print(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            content = self._remove_duplicates(content)
            print(f"üìä –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            content = self._filter_relevant_content(content)
            print(f"üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –≠—Ç–∞–ø 4: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
        content = self._normalize_text(content)
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return content
    
    def get_content_stats(self, content: str) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return {"words": 0, "paragraphs": 0, "sentences": 0, "chars": 0}
            
        words = len(re.findall(r'\b[–∞-—è—ëa-z]+\b', content.lower()))
        paragraphs = len([p for p in content.split('\n\n') if p.strip()])
        sentences = len(re.findall(r'[.!?]+', content))
        chars = len(content)
        
        return {
            "words": words,
            "paragraphs": paragraphs, 
            "sentences": sentences,
            "chars": chars
        }