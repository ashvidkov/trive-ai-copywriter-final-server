"""
FastAPI —Ä–æ—É—Ç–µ—Ä –¥–ª—è SEO Copywriter –º–æ–¥—É–ª—è
"""

import time
import requests
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import json
from datetime import datetime
from search_service import YandexSearchService, SearchResult
from content_parser import ContentParser, ArticleContent
from openai_service import OpenAIService, GeneratedArticle
from text_ru_service import TextRuService


class SearchRequest(BaseModel):
    query: str
    page_size: Optional[int] = 10
    region: Optional[int] = 225  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏


class SearchResultResponse(BaseModel):
    title: str
    url: str
    snippet: str


class SearchResponse(BaseModel):
    query: str
    results: List[SearchResultResponse]
    total_found: int


class CleaningSettings(BaseModel):
    mode: Optional[str] = "automatic"  # "automatic" –∏–ª–∏ "manual"
    remove_technical_blocks: Optional[bool] = True
    remove_duplicates: Optional[bool] = True
    filter_relevance: Optional[bool] = True
    min_paragraph_length: Optional[int] = 50
    max_paragraph_length: Optional[int] = 2000
    relevance_threshold: Optional[float] = 0.7

class ParseRequest(BaseModel):
    urls: List[str]
    delay: Optional[float] = 1.0
    enable_cleaning: Optional[bool] = True
    enable_chunking: Optional[bool] = False
    chunk_size: Optional[int] = 1000
    chunking_method: Optional[str] = "paragraphs"
    cleaning_settings: Optional[CleaningSettings] = None


class ArticleContentResponse(BaseModel):
    url: str
    title: str
    content: str
    cleaned_content: str
    meta_description: str
    word_count: int
    cleaned_word_count: int
    content_stats: dict
    chunks: List[dict]
    chunking_stats: dict


class ParseError(BaseModel):
    url: str
    error: str
    error_type: str  # "request_error", "parsing_error", "content_too_short"

class ParseResponse(BaseModel):
    parsed_articles: List[ArticleContentResponse]
    failed_urls: List[ParseError]
    total_requested: int
    total_parsed: int
    total_failed: int
    success_rate: float


class ChunkWithPriority(BaseModel):
    text: str
    word_count: int
    article_index: int
    article_title: str
    keywords: List[str]
    priority: str  # 'low', 'medium', 'high'
    excluded: bool = False

class ModelSettings(BaseModel):
    system_prompt: Optional[str] = "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –∏ –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ SEO-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π –±–æ–ª—å—à–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤ –∏ —Ü–∏—Ñ—Ä. –°–æ–∑–¥–∞–π —Å—Ç–∞—Ç—å—é —Å –≥–ª—É–±–æ–∫–æ–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (H1-H6), –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞, –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞. –§–æ–∫—É—Å –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º SEO –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏."
    user_prompt: Optional[str] = "–ü–µ—Ä–µ–ø–∏—à–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç, —Å–¥–µ–ª–∞–≤ –µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–≤ –≤—Å—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–º—ã—Å–ª:"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000
    model: Optional[str] = "gpt-4o"
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏
    plan_temperature: Optional[float] = 0.2
    plan_max_tokens: Optional[int] = 3000
    plan_system_prompt: Optional[str] = "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ü–†–û–ß–ò–¢–ê–¢–¨ –∏ –ü–†–û–ê–ù–ê–õ–ò–ó–ò–†–û–í–ê–¢–¨ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏.\n\n–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n1. –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ—á–∏—Ç–∞–π –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ\n2. –í–´–î–ï–õ–ò –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ–º—ã –∏ –ø–æ–¥—Ç–µ–º—ã –≤ –∫–∞–∂–¥–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ\n3. –°–û–ó–î–ê–ô –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –†–ï–ê–õ–¨–ù–û–ì–û —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ\n4. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —à–∞–±–ª–æ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏\n\n–ó–ê–ü–†–ï–©–ï–ù–û:\n‚ùå \"–í–≤–µ–¥–µ–Ω–∏–µ\", \"–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\", \"–ó–∞–∫–ª—é—á–µ–Ω–∏–µ\"\n‚ùå \"–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã\", \"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\"\n‚ùå –õ—é–±—ã–µ –æ–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–µ–∑ —Å–≤—è–∑–∏ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º\n\n–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:\n‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞\n‚úÖ –°–æ–∑–¥–∞–≤–∞–π –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ–º\n‚úÖ –£–∫–∞–∑—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞"
    plan_user_prompt: Optional[str] = "–ê–ù–ê–õ–ò–ó–ò–†–£–ô —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞–π –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏.\n\n–°–ù–ê–ß–ê–õ–ê –ø—Ä–æ—á–∏—Ç–∞–π –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ –≤—ã–¥–µ–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ–º—ã.\n\n–°–û–ó–î–ê–ô –ø–ª–∞–Ω —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –†–ï–ê–õ–¨–ù–û–ì–û —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.\n–ö–∞–∂–¥—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É –∏–∑ —Ç–µ–∫—Å—Ç–∞.\n\n–ü–†–ò–ú–ï–†–´ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\n‚úÖ \"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ö–æ–ª–æ–¥–Ω–æ–π –≤—ã—Å–∞–¥–∫–∏ –º–µ—Ç–∏–∑–æ–≤\"\n‚úÖ \"–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –∏ —ç—Ç–∞–ø—ã –ø—Ä–æ—Ü–µ—Å—Å–∞\"\n‚úÖ \"–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –∏—Ö —Å–≤–æ–π—Å—Ç–≤–∞\"\n‚úÖ \"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\"\n‚úÖ \"–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–∏\"\n\n–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∏–ø–∞ \"–í–≤–µ–¥–µ–Ω–∏–µ\" –∏–ª–∏ \"–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\"."

class ArticlePlan(BaseModel):
    plan_text: str        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏
    format: str = "text"  # –§–æ—Ä–º–∞—Ç –ø–ª–∞–Ω–∞

class GenerateRequest(BaseModel):
    topic: str
    source_urls: List[str]
    target_length: Optional[str] = "medium"  # "short", "medium", "long"
    chunk_size: Optional[int] = 800
    enable_cleaning: Optional[bool] = True
    model_settings: Optional[ModelSettings] = None
    chunks_with_priorities: Optional[List[ChunkWithPriority]] = None

class GeneratePlanRequest(BaseModel):
    chunks_with_priorities: List[ChunkWithPriority]
    model_settings: Optional[ModelSettings] = None

class GeneratePlanResponse(BaseModel):
    article_plan: ArticlePlan

# –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ª–æ–≥–∏–∫–∏
class ChunkForAdvancedProcessing(BaseModel):
    text: str
    priority: str  # 'high', 'medium', 'low', 'exclude'

class AdvancedPlanRequest(BaseModel):
    chunks: List[ChunkForAdvancedProcessing]

class AdvancedPlanResponse(BaseModel):
    plan_text: str
    format: str = "text"
    total_chunks_processed: int
    chunks_with_summaries: List[Dict]

# –ú–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ –ø–ª–∞–Ω—É
class GenerateFromPlanRequest(BaseModel):
    plan_text: str
    keywords: List[str]
    target_length_chars: int
    system_prompt: Optional[str] = None
    temperature: Optional[float] = 0.5  # –≠—Ç–∞–ª–æ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∏–∑ Jupyter Notebook
    
    # –ù–æ–≤—ã–µ SEO-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    additional_keywords: Optional[str] = ""
    lsi_keywords: Optional[str] = ""
    target_phrases: Optional[str] = ""
    keyword_density: Optional[float] = 2.5
    seo_prompt: Optional[str] = "üîπ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø SEO-–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø –¢–ï–•–ù–ò–ß–ï–°–ö–û–ì–û –ö–û–ù–¢–ï–ù–¢–ê:\n- –°–æ–∑–¥–∞–≤–∞–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–∞–ª—è–º–∏ –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º–∏\n- –í–∫–ª—é—á–∞–π —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\n- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã\n- –î–æ–±–∞–≤–ª—è–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–µ–π—Å—ã, –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n- –í–∫–ª—é—á–∞–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏ —Ç—Ä–µ–Ω–¥—ã —Ä–∞–∑–≤–∏—Ç–∏—è\n- –°–æ–∑–¥–∞–≤–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é\n- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n- –í–∫–ª—é—á–∞–π LSI-–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã\n- –î–æ–±–∞–≤–ª—è–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n- –°–æ–∑–¥–∞–≤–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –ø–æ–∏—Å–∫–æ–≤—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
    writing_style: Optional[str] = "technical"
    content_type: Optional[str] = "educational"
    target_audience: Optional[str] = "specialists"
    heading_type: Optional[str] = "h2"
    paragraph_length: Optional[str] = "medium"
    use_lists: Optional[bool] = True
    internal_links: Optional[bool] = False


class GeneratedArticleResponse(BaseModel):
    title: str
    h1: str
    content: str
    meta_description: str
    keywords: str
    article_plan: Optional[ArticlePlan] = None
    word_count: int
    source_chunks_count: int
    source_urls: List[str]


router = APIRouter(prefix="/seo", tags=["SEO Copywriter"])


@router.post("/search", response_model=SearchResponse)
async def search_yandex(request: SearchRequest):
    """
    –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex Search API
    """
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
    
    try:
        search_service = YandexSearchService()
        results = search_service.search(request.query, request.page_size, request.region)
        
        response_results = [
            SearchResultResponse(
                title=result.title,
                url=result.url,
                snippet=result.snippet
            )
            for result in results
        ]
        
        return SearchResponse(
            query=request.query,
            results=response_results,
            total_found=len(response_results)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}")


@router.post("/parse", response_model=ParseResponse)
async def parse_articles(request: ParseRequest):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç–µ–π –ø–æ —Å–ø–∏—Å–∫—É URL
    """
    if not request.urls:
        raise HTTPException(status_code=400, detail="–°–ø–∏—Å–æ–∫ URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
    
    if len(request.urls) > 20:
        raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 20 URL –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å")
    
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—á–∏—Å—Ç–∫–∏
        cleaning_settings = {}
        if request.cleaning_settings:
            cleaning_settings = {
                'mode': request.cleaning_settings.mode,
                'remove_technical_blocks': request.cleaning_settings.remove_technical_blocks,
                'remove_duplicates': request.cleaning_settings.remove_duplicates,
                'filter_relevance': request.cleaning_settings.filter_relevance,
                'min_paragraph_length': request.cleaning_settings.min_paragraph_length,
                'max_paragraph_length': request.cleaning_settings.max_paragraph_length,
                'relevance_threshold': request.cleaning_settings.relevance_threshold
            }
        
        parser = ContentParser(
            enable_cleaning=request.enable_cleaning,
            enable_chunking=request.enable_chunking, 
            chunk_size=request.chunk_size,
            cleaning_settings=cleaning_settings
        )
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–∞—Ö
        articles = []
        failed_urls = []
        
        for url in request.urls:
            try:
                article = parser.parse_article(url)
                if article:
                    articles.append(article)
                else:
                    failed_urls.append(ParseError(
                        url=url,
                        error="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞)",
                        error_type="content_too_short"
                    ))
            except requests.RequestException as e:
                failed_urls.append(ParseError(
                    url=url,
                    error=f"–û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                    error_type="request_error"
                ))
            except Exception as e:
                failed_urls.append(ParseError(
                    url=url,
                    error=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}",
                    error_type="parsing_error"
                ))
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if request.delay and request.delay > 0:
                time.sleep(request.delay)
        
        response_articles = [
            ArticleContentResponse(
                url=article.url,
                title=article.title,
                content=article.content,
                cleaned_content=article.cleaned_content,
                meta_description=article.meta_description,
                word_count=article.word_count,
                cleaned_word_count=article.cleaned_word_count,
                content_stats=article.content_stats,
                chunks=[chunk.to_dict() for chunk in article.chunks],
                chunking_stats=article.chunking_stats
            )
            for article in articles
        ]
        
        total_requested = len(request.urls)
        total_parsed = len(response_articles)
        total_failed = len(failed_urls)
        success_rate = (total_parsed / total_requested) * 100 if total_requested > 0 else 0
        
        return ParseResponse(
            parsed_articles=response_articles,
            failed_urls=failed_urls,
            total_requested=total_requested,
            total_parsed=total_parsed,
            total_failed=total_failed,
            success_rate=success_rate
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")


@router.post("/generate-plan", response_model=GeneratePlanResponse)
async def generate_article_plan(request: GeneratePlanRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏: –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ‚Üí —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è ‚Üí –ø–ª–∞–Ω"""
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏ —Å –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π")
    print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(request.chunks_with_priorities)}")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
        openai_service = OpenAIService()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ, —á—Ç–æ –ø–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ excluded –∏–ª–∏ "–∏—Å–∫–ª—é—á–∏—Ç—å")
        chunks_for_processing = [chunk for chunk in request.chunks_with_priorities if not chunk.excluded and chunk.priority != "–∏—Å–∫–ª—é—á–∏—Ç—å"]
        print(f"üìù –ß–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(chunks_for_processing)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        priority_counts = {}
        for chunk in chunks_for_processing:
            priority_counts[chunk.priority] = priority_counts.get(chunk.priority, 0) + 1
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {priority_counts}")
        
        # –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        print("üî¢ –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤...")
        for i, chunk in enumerate(chunks_for_processing):
            try:
                embedding = await openai_service.embed_chunk(chunk.text)
                print(f"  ‚úÖ –ß–∞–Ω–∫ {i+1} ({chunk.priority}): –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω ({len(embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π)")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}")
        
        # –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        print("üìù –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É...")
        chunks_with_summaries = []
        
        # –ú–∞–ø–ø–∏–Ω–≥ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º)
        priority_mapping = {
            "high": "–≤—ã—Å–æ–∫–∏–π",
            "medium": "—Å—Ä–µ–¥–Ω–∏–π", 
            "low": "–Ω–∏–∑–∫–∏–π",
            "exclude": "–∏—Å–∫–ª—é—á–∏—Ç—å"
        }
        
        for i, chunk in enumerate(chunks_for_processing):
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤ —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                russian_priority = priority_mapping.get(chunk.priority, chunk.priority)
                print(f"  üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1} —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º '{chunk.priority}' -> '{russian_priority}'...")
                
                summary = await openai_service.summarize_with_priority(chunk.text, russian_priority)
                chunks_with_summaries.append({
                    "text": chunk.text,
                    "priority": russian_priority,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä—É—Å—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    "summary": summary
                })
                print(f"  ‚úÖ –ß–∞–Ω–∫ {i+1} ({russian_priority}): {summary[:100]}...")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}")
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –±–µ–∑ —Å—É–º–º–∞—Ä–∏–∏
                chunks_with_summaries.append({
                    "text": chunk.text,
                    "priority": russian_priority,
                    "summary": f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
                })
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–π
        print("ü§ñ –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏...")
        plan_text = await openai_service.generate_article_plan_from_summaries(chunks_with_summaries)
        
        print(f"‚úÖ –ü–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(plan_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –ù–∞—á–∞–ª–æ –ø–ª–∞–Ω–∞: {plan_text[:200]}...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        plan_result = {
            "plan_text": plan_text,
            "format": "text"
        }
        
        return GeneratePlanResponse(article_plan=plan_result)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ generate_article_plan: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {str(e)}")

@router.post("/generate", response_model=GeneratedArticleResponse)
async def generate_article(request: GenerateRequest):
    """
    –ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏: –ø–æ–∏—Å–∫ ‚Üí –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí –æ—á–∏—Å—Ç–∫–∞ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí GPT –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    """
    if not request.topic.strip():
        raise HTTPException(status_code=400, detail="–¢–µ–º–∞ —Å—Ç–∞—Ç—å–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π")
    
    if not request.source_urls:
        raise HTTPException(status_code=400, detail="–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω URL")
    
    if len(request.source_urls) > 10:
        raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 10 URL –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å")
    
    try:
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–º–µ: {request.topic}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –∏—Ö —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥
        if request.chunks_with_priorities:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —á–∞–Ω–∫–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏–∑ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
            chunks_with_priorities = request.chunks_with_priorities
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            filtered_chunks = [chunk for chunk in chunks_with_priorities if not chunk.excluded]
            all_chunks = [chunk.text for chunk in filtered_chunks]
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(filtered_chunks)} —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ (–∏—Å–∫–ª—é—á–µ–Ω–æ: {len(chunks_with_priorities) - len(filtered_chunks)})")
        else:
            # –°—Ç–∞—Ä—ã–π –ø—É—Ç—å: –ø–∞—Ä—Å–∏–Ω–≥ –∏ —á–∞–Ω–∫–∏–Ω–≥
            parser = ContentParser(
                enable_cleaning=request.enable_cleaning,
                enable_chunking=True,
                chunk_size=request.chunk_size
            )
            articles = parser.parse_multiple_articles(request.source_urls, delay=1.0)
            
            if not articles:
                raise HTTPException(status_code=404, detail="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
            all_chunks = []
            for article in articles:
                if article.chunks:
                    chunks_content = [chunk.content for chunk in article.chunks]
                    all_chunks.extend(chunks_content)
                else:
                    if article.cleaned_content:
                        all_chunks.append(article.cleaned_content)
            
            if not all_chunks:
                raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            
            print(f"üìä –°–æ–±—Ä–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏
        openai_service = OpenAIService()
        model_settings = request.model_settings or ModelSettings()
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        article_plan = None
        if request.chunks_with_priorities:
            chunks_with_priorities = request.chunks_with_priorities
            filtered_chunks = [chunk for chunk in chunks_with_priorities if not chunk.excluded]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞
            chunks_data = []
            for i, chunk in enumerate(filtered_chunks):
                priority_label = "high" if chunk.priority == "high" else "low" if chunk.priority == "low" else "medium"
                chunks_data.append(f"{i+1}: \"{priority_label}: {chunk.text[:200]}...\"")
            
            chunks_with_priority = "\n".join(chunks_data)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç—ã –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏
            plan_prompt = model_settings.plan_user_prompt.format(
                chunks_with_priority=chunks_with_priority,
                theme=request.topic,
                style="–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π"
            )
            
            try:
                plan_response = await openai_service._call_openai(
                    messages=[
                        {"role": "system", "content": model_settings.plan_system_prompt},
                        {"role": "user", "content": plan_prompt}
                    ],
                    temperature=model_settings.plan_temperature,
                    max_tokens=model_settings.plan_max_tokens,
                    model=model_settings.model
                )
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                import json
                plan_data = json.loads(plan_response)
                article_plan = ArticlePlan(
                    sections=plan_data.get("sections", []),
                    chunk_mapping={},
                    total_sections=plan_data.get("total_sections", 0)
                )
                
                # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —á–∞–Ω–∫–æ–≤ –∫ —Ä–∞–∑–¥–µ–ª–∞–º
                for section in article_plan.sections:
                    for chunk_num in section.get("chunks", []):
                        if chunk_num - 1 < len(filtered_chunks):
                            article_plan.chunk_mapping[chunk_num - 1] = section["title"]
                
                print(f"üìã –ü–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å–æ–∑–¥–∞–Ω: {article_plan.total_sections} —Ä–∞–∑–¥–µ–ª–æ–≤")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏: {str(e)}")
                article_plan = None
        
        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ OpenAI
        generated_article = await openai_service.generate_article_from_chunks(
            chunks=all_chunks,
            topic=request.topic,
            target_length=request.target_length,
            system_prompt=model_settings.system_prompt,
            user_prompt=model_settings.user_prompt,
            temperature=model_settings.temperature,
            max_tokens=model_settings.max_tokens,
            model=model_settings.model
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = GeneratedArticleResponse(
            title=generated_article.title,
            h1=generated_article.h1,
            content=generated_article.content,
            meta_description=generated_article.meta_description,
            keywords=generated_article.keywords,
            article_plan=article_plan,
            word_count=generated_article.word_count,
            source_chunks_count=len(all_chunks),
            source_urls=request.source_urls
        )
        
        print(f"‚úÖ –°—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {response.word_count} —Å–ª–æ–≤")
        return response
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {str(e)}")


@router.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ SEO –º–æ–¥—É–ª—è"""
    return {"status": "ok", "module": "SEO Copywriter"}

@router.post("/generate-advanced-plan", response_model=AdvancedPlanResponse)
async def generate_advanced_article_plan(request: AdvancedPlanRequest):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ª–æ–≥–∏–∫–∏:
    1. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
    2. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–π
    """
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏")
    print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(request.chunks)}")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
        openai_service = OpenAIService()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ, —á—Ç–æ –ø–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ "exclude" –∏–ª–∏ "–∏—Å–∫–ª—é—á–∏—Ç—å")
        chunks_for_processing = [chunk for chunk in request.chunks if chunk.priority not in ["exclude", "–∏—Å–∫–ª—é—á–∏—Ç—å"]]
        print(f"üìù –ß–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(chunks_for_processing)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        priority_counts = {}
        for chunk in chunks_for_processing:
            priority_counts[chunk.priority] = priority_counts.get(chunk.priority, 0) + 1
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {priority_counts}")
        
        # –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        print("üî¢ –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤...")
        for i, chunk in enumerate(chunks_for_processing):
            try:
                embedding = await openai_service.embed_chunk(chunk.text)
                print(f"  ‚úÖ –ß–∞–Ω–∫ {i+1} ({chunk.priority}): –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω ({len(embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π)")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}")
        
        # –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        print("üìù –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É...")
        chunks_with_summaries = []
        
        # –ú–∞–ø–ø–∏–Ω–≥ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º)
        priority_mapping = {
            "high": "–≤—ã—Å–æ–∫–∏–π",
            "medium": "—Å—Ä–µ–¥–Ω–∏–π", 
            "low": "–Ω–∏–∑–∫–∏–π",
            "exclude": "–∏—Å–∫–ª—é—á–∏—Ç—å"
        }
        
        for i, chunk in enumerate(chunks_for_processing):
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤ —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                russian_priority = priority_mapping.get(chunk.priority, chunk.priority)
                print(f"  üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1} —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º '{chunk.priority}' -> '{russian_priority}'...")
                
                summary = await openai_service.summarize_with_priority(chunk.text, russian_priority)
                chunks_with_summaries.append({
                    "text": chunk.text,
                    "priority": russian_priority,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä—É—Å—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    "summary": summary
                })
                print(f"  ‚úÖ –ß–∞–Ω–∫ {i+1} ({russian_priority}): {summary[:100]}...")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}")
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –±–µ–∑ —Å—É–º–º–∞—Ä–∏–∏
                chunks_with_summaries.append({
                    "text": chunk.text,
                    "priority": russian_priority,
                    "summary": f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
                })
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–π
        print("ü§ñ –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏...")
        plan_text = await openai_service.generate_article_plan_from_summaries(chunks_with_summaries)
        
        print(f"‚úÖ –ü–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(plan_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –ù–∞—á–∞–ª–æ –ø–ª–∞–Ω–∞: {plan_text[:200]}...")
        
        return AdvancedPlanResponse(
            plan_text=plan_text,
            format="text",
            total_chunks_processed=len(chunks_for_processing),
            chunks_with_summaries=chunks_with_summaries
        )
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ generate_advanced_article_plan: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {str(e)}")

@router.post("/generate-from-plan", response_model=GeneratedArticleResponse)
async def generate_article_from_plan(request: GenerateFromPlanRequest):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ–≥–æ –ø–ª–∞–Ω–∞
    """
    print(f"üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞")
    print(f"üìä –î–ª–∏–Ω–∞ –ø–ª–∞–Ω–∞: {len(request.plan_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üéØ –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞: {request.target_length_chars} —Å–∏–º–≤–æ–ª–æ–≤")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
        openai_service = OpenAIService()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ —Å –Ω–æ–≤—ã–º–∏ SEO-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        generated_article = await openai_service.generate_full_article_from_plan(
            plan_text=request.plan_text,
            keywords=request.keywords,
            target_length_chars=request.target_length_chars,
            system_prompt=request.system_prompt,
            temperature=request.temperature,
            # –ù–æ–≤—ã–µ SEO-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            additional_keywords=request.additional_keywords,
            lsi_keywords=request.lsi_keywords,
            target_phrases=request.target_phrases,
            keyword_density=request.keyword_density,
            seo_prompt=request.seo_prompt,
            writing_style=request.writing_style,
            content_type=request.content_type,
            target_audience=request.target_audience,
            heading_type=request.heading_type,
            paragraph_length=request.paragraph_length,
            use_lists=request.use_lists,
            internal_links=request.internal_links
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = GeneratedArticleResponse(
            title=generated_article.title,
            h1=generated_article.h1,
            content=generated_article.content,
            meta_description=generated_article.meta_description,
            keywords=generated_article.keywords,
            article_plan=None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–∞–Ω –≤ –æ—Ç–≤–µ—Ç–µ
            word_count=generated_article.word_count,
            source_chunks_count=generated_article.source_chunks_count,
            source_urls=[]  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º URL –≤ —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ
        )
        
        print(f"‚úÖ –°—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {response.word_count} —Å–ª–æ–≤")
        return response
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {str(e)}")

@router.post("/generate-from-plan-stream")
async def generate_article_from_plan_stream(request: GenerateFromPlanRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π –ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ SSE"""
    
    async def generate_article_with_logs():
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –ª–æ–≥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ª–æ–≥
            yield f"data: {json.dumps({'log': 'üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            yield f"data: {json.dumps({'log': f'üìä –î–ª–∏–Ω–∞ –ø–ª–∞–Ω–∞: {len(request.plan_text)} —Å–∏–º–≤–æ–ª–æ–≤', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            yield f"data: {json.dumps({'log': f'üéØ –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞: {request.target_length_chars} —Å–∏–º–≤–æ–ª–æ–≤', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
            openai_service = OpenAIService()
            
            yield f"data: {json.dumps({'log': 'ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI —Å–µ—Ä–≤–∏—Å–∞...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ —Å –Ω–æ–≤—ã–º–∏ SEO-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            yield f"data: {json.dumps({'log': 'üìù –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç—å–∏...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            generated_article = await openai_service.generate_full_article_from_plan(
                plan_text=request.plan_text,
                keywords=request.keywords,
                target_length_chars=request.target_length_chars,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                # –ù–æ–≤—ã–µ SEO-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                additional_keywords=request.additional_keywords,
                lsi_keywords=request.lsi_keywords,
                target_phrases=request.target_phrases,
                keyword_density=request.keyword_density,
                seo_prompt=request.seo_prompt,
                writing_style=request.writing_style,
                content_type=request.content_type,
                target_audience=request.target_audience,
                heading_type=request.heading_type,
                paragraph_length=request.paragraph_length,
                use_lists=request.use_lists,
                internal_links=request.internal_links
            )
            
            yield f"data: {json.dumps({'log': f'‚úÖ –°—Ç–∞—Ç—å—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {generated_article.word_count} —Å–ª–æ–≤, {len(generated_article.content)} —Å–∏–º–≤–æ–ª–æ–≤', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = GeneratedArticleResponse(
                title=generated_article.title,
                h1=generated_article.h1,
                content=generated_article.content,
                meta_description=generated_article.meta_description,
                keywords=generated_article.keywords,
                article_plan=None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–∞–Ω –≤ –æ—Ç–≤–µ—Ç–µ
                word_count=generated_article.word_count,
                source_chunks_count=generated_article.source_chunks_count,
                source_urls=[]  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º URL –≤ —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            yield f"data: {json.dumps({'result': response.dict(), 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {str(e)}"
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ SSE –ø–æ—Ç–æ–∫–µ: {str(e)}")
            try:
                yield f"data: {json.dumps({'error': error_msg, 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            except Exception as stream_error:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—à–∏–±–∫–∏ –≤ –ø–æ—Ç–æ–∫: {str(stream_error)}")
                yield f"data: {json.dumps({'error': '–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_article_with_logs(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )

@router.post("/generate-plan-stream")
async def generate_article_plan_stream(request: GeneratePlanRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π –ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ SSE"""
    
    async def generate_plan_with_logs():
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –ª–æ–≥–æ–≤"""
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –ª–æ–≥
            yield f"data: {json.dumps({'log': 'üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏ —Å –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            yield f"data: {json.dumps({'log': f'üìä –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(request.chunks_with_priorities)}', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI —Å–µ—Ä–≤–∏—Å
            openai_service = OpenAIService()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫–∏
            chunks_for_processing = [chunk for chunk in request.chunks_with_priorities if not chunk.excluded and chunk.priority != "–∏—Å–∫–ª—é—á–∏—Ç—å"]
            yield f"data: {json.dumps({'log': f'üìù –ß–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(chunks_for_processing)}', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            priority_counts = {}
            for chunk in chunks_for_processing:
                priority_counts[chunk.priority] = priority_counts.get(chunk.priority, 0) + 1
            yield f"data: {json.dumps({'log': f'üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {priority_counts}', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
            yield f"data: {json.dumps({'log': 'üî¢ –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            for i, chunk in enumerate(chunks_for_processing):
                try:
                    embedding = await openai_service.embed_chunk(chunk.text)
                    yield f"data: {json.dumps({'log': f'  ‚úÖ –ß–∞–Ω–∫ {i+1} ({chunk.priority}): –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω ({len(embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π)', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                except Exception as e:
                    yield f"data: {json.dumps({'log': f'  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
            
            # –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            yield f"data: {json.dumps({'log': 'üìù –®–∞–≥ 2: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            chunks_with_summaries = []
            priority_mapping = {
                "high": "–≤—ã—Å–æ–∫–∏–π",
                "medium": "—Å—Ä–µ–¥–Ω–∏–π", 
                "low": "–Ω–∏–∑–∫–∏–π",
                "exclude": "–∏—Å–∫–ª—é—á–∏—Ç—å"
            }
            
            for i, chunk in enumerate(chunks_for_processing):
                try:
                    russian_priority = priority_mapping.get(chunk.priority, chunk.priority)
                    yield f"data: {json.dumps({'log': f'  üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1} —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º \'{chunk.priority}\' -> \'{russian_priority}\'...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                    
                    summary = await openai_service.summarize_with_priority(chunk.text, russian_priority)
                    chunks_with_summaries.append({
                        "text": chunk.text,
                        "priority": russian_priority,
                        "summary": summary
                    })
                    yield f"data: {json.dumps({'log': f'  ‚úÖ –ß–∞–Ω–∫ {i+1} ({russian_priority}): {summary[:100]}...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                except Exception as e:
                    yield f"data: {json.dumps({'log': f'  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–∞ {i+1}: {str(e)}', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                    chunks_with_summaries.append({
                        "text": chunk.text,
                        "priority": russian_priority,
                        "summary": f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
                    })
            
            # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–π
            yield f"data: {json.dumps({'log': 'ü§ñ –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å—Ç–∞—Ç—å–∏...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            plan_text = await openai_service.generate_article_plan_from_summaries(chunks_with_summaries)
            
            yield f"data: {json.dumps({'log': f'‚úÖ –ü–ª–∞–Ω —Å—Ç–∞—Ç—å–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(plan_text)} —Å–∏–º–≤–æ–ª–æ–≤', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            yield f"data: {json.dumps({'log': f'üìã –ù–∞—á–∞–ª–æ –ø–ª–∞–Ω–∞: {plan_text[:200]}...', 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            plan_result = {
                "plan_text": plan_text,
                "format": "text"
            }
            yield f"data: {json.dumps({'result': plan_result, 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≤ generate_article_plan: {str(e)}"
            yield f"data: {json.dumps({'error': error_msg, 'timestamp': datetime.now().isoformat()}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_plan_with_logs(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )


# ============================================================================
# –≠–ù–î–ü–û–ò–ù–¢–´ –î–õ–Ø –ü–†–û–í–ï–†–ö–ò SEO-–ö–ê–ß–ï–°–¢–í–ê –ß–ï–†–ï–ó TEXT.RU API
# ============================================================================

class SeoQualityCheckRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É SEO-–∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
    text: str
    max_wait_time: Optional[int] = 120  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö


class SeoQualityMetrics(BaseModel):
    """–ú–µ—Ç—Ä–∏–∫–∏ SEO-–∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
    uniqueness: float  # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (100% - –≤–æ–¥–Ω–æ—Å—Ç—å)
    water_percent: float  # –ü—Ä–æ—Ü–µ–Ω—Ç –≤–æ–¥—ã
    spam_percent: float  # –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞—Å–ø–∞–º–ª–µ–Ω–Ω–æ—Å—Ç–∏
    char_count: int  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
    word_count: int  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    spell_errors: int  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è text.ru)


class SeoQualityResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ SEO-–∫–∞—á–µ—Å—Ç–≤–∞"""
    status: str  # "success", "error", "timeout"
    ready: bool
    metrics: Optional[SeoQualityMetrics] = None
    keywords: List[Dict[str, object]] = []  # –¢–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    spell_check: List[Dict[str, object]] = []  # –û—à–∏–±–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è
    error_code: Optional[str] = None
    error_desc: Optional[str] = None


@router.post("/check-seo-quality", response_model=SeoQualityResponse)
async def check_seo_quality(request: SeoQualityCheckRequest):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç SEO-–∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ API text.ru
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
    - –ü—Ä–æ—Ü–µ–Ω—Ç –≤–æ–¥—ã –∏ –∑–∞—Å–ø–∞–º–ª–µ–Ω–Ω–æ—Å—Ç–∏
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å–ª–æ–≤
    - –û—à–∏–±–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è
    - –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        import os
        text_ru_api_key = os.getenv("TEXT_RU_API_KEY", "ff355ac3b34a87295a0b78617a407477")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å
        text_ru_service = TextRuService(text_ru_api_key)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å
        balance = text_ru_service.get_balance()
        if balance <= 0:
            return SeoQualityResponse(
                status="error",
                ready=False,
                error_code="no_balance",
                error_desc="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ text.ru"
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if len(request.text) < 100:
            return SeoQualityResponse(
                status="error",
                ready=False,
                error_code="text_too_short",
                error_desc="–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤"
            )
        
        if len(request.text) > 150000:
            return SeoQualityResponse(
                status="error",
                ready=False,
                error_code="text_too_long",
                error_desc="–¢–µ–∫—Å—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å 150000 —Å–∏–º–≤–æ–ª–æ–≤"
            )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞
        results = text_ru_service.check_text_quality(request.text, request.max_wait_time)
        
        if results['status'] == 'error':
            return SeoQualityResponse(
                status="error",
                ready=False,
                error_code=results.get('error_code', 'unknown'),
                error_desc=results.get('error_desc', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
            )
        
        if results['status'] == 'timeout':
            return SeoQualityResponse(
                status="timeout",
                ready=False,
                error_code="timeout",
                error_desc=f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({request.max_wait_time}—Å)"
            )
        
        if results['status'] == 'pending':
            return SeoQualityResponse(
                status="pending",
                ready=False,
                error_code="pending",
                error_desc="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤—ã, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ"
            )
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        formatted_results = text_ru_service.format_seo_results(results)
        
        if formatted_results['status'] == 'success':
            summary = formatted_results['summary']
            details = formatted_results['details']
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ SEO-–∞–Ω–∞–ª–∏–∑–∞
            keywords = []
            for keyword_info in details.get('keywords', []):
                keywords.append({
                    'keyword': keyword_info['keyword'],
                    'count': keyword_info['count']
                })
            
            return SeoQualityResponse(
                status="success",
                ready=True,
                metrics=SeoQualityMetrics(
                    uniqueness=summary['uniqueness'],
                    water_percent=summary['water_percent'],
                    spam_percent=summary['spam_percent'],
                    char_count=summary['char_count'],
                    word_count=summary['word_count'],
                    spell_errors=0  # text.ru –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–∏
                ),
                keywords=keywords,
                spell_check=[]  # text.ru –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–∏
            )
        else:
            return SeoQualityResponse(
                status="error",
                ready=False,
                error_code="format_error",
                error_desc="–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
            )
            
    except Exception as e:
        return SeoQualityResponse(
            status="error",
            ready=False,
            error_code="exception",
            error_desc=f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}"
        )


@router.get("/text-ru-balance")
async def get_text_ru_balance():
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ—Å—Ç–∞—Ç–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ text.ru
    """
    try:
        import os
        
        text_ru_api_key = os.getenv("TEXT_RU_API_KEY", "ff355ac3b34a87295a0b78617a407477")
        text_ru_service = TextRuService(text_ru_api_key)
        
        balance = text_ru_service.get_balance()
        
        return {
            "status": "success",
            "balance": balance,
            "balance_formatted": f"{balance:,}".replace(",", " ")
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "balance": 0
        }