"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è GPT –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import re
import math
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class TextChunk:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    content: str
    chunk_id: int
    start_position: int
    end_position: int
    word_count: int
    sentence_count: int
    keywords: List[str]
    
    def to_dict(self) -> Dict:
        return {
            "content": self.content,
            "chunk_id": self.chunk_id,
            "start_position": self.start_position,
            "end_position": self.end_position,
            "word_count": self.word_count,
            "sentence_count": self.sentence_count,
            "keywords": self.keywords
        }


class TextChunker:
    def __init__(self, target_chunk_size: int = 1000, overlap_size: int = 100):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞
        
        Args:
            target_chunk_size: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö
            overlap_size: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–ª–æ–≤–∞—Ö
        """
        self.target_chunk_size = target_chunk_size
        self.overlap_size = overlap_size
        self.min_chunk_size = max(200, target_chunk_size // 4)
        self.max_chunk_size = target_chunk_size * 2
        
    def _extract_keywords(self, text: str, limit: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ/–≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å
        technical_patterns = [
            r'\b(–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ|—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è|–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ|–ø—Ä–æ—Ü–µ—Å—Å|–º–µ—Ç–æ–¥|—Å–∏—Å—Ç–µ–º–∞|–º–∞—Ç–µ—Ä–∏–∞–ª)\b',
            r'\b(–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ|–æ–±—Ä–∞–±–æ—Ç–∫–∞|–∫–∞—á–µ—Å—Ç–≤–æ|—Å—Ç–∞–Ω–¥–∞—Ä—Ç|—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ|–Ω–æ—Ä–º–∞)\b',
            r'\b(–º–µ—Ç–∞–ª–ª|—Å—Ç–∞–ª—å|—á—É–≥—É–Ω|–∂–µ–ª–µ–∑–æ|–∞–ª—é–º–∏–Ω–∏–π|–º–µ–¥—å|—Ü–∏–Ω–∫|–æ–ª–æ–≤–æ)\b'
        ]
        
        technical_words = []
        for pattern in technical_patterns:
            technical_words.extend(re.findall(pattern, text.lower()))
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã
        word_freq = {}
        for word in words + technical_words * 2:  # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤–µ—Å—è—Ç –±–æ–ª—å—à–µ
            if len(word) >= 4:  # –ú–∏–Ω–∏–º—É–º 4 —Å–∏–º–≤–æ–ª–∞
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Å–ª–æ–≤–∞
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:limit] if freq > 1]
    
    def _split_by_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+(?=[–ê-–Ø–Å])', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _split_by_paragraphs(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∞–±–∑–∞—Ü—ã"""
        paragraphs = text.split('\n\n')
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _get_word_count(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤"""
        return len(re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower()))
    
    def _create_chunk_from_sentences(self, sentences: List[str], chunk_id: int, 
                                   start_pos: int) -> TextChunk:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        content = ' '.join(sentences)
        word_count = self._get_word_count(content)
        sentence_count = len(sentences)
        keywords = self._extract_keywords(content)
        
        return TextChunk(
            content=content,
            chunk_id=chunk_id,
            start_position=start_pos,
            end_position=start_pos + len(content),
            word_count=word_count,
            sentence_count=sentence_count,
            keywords=keywords
        )
    
    def chunk_text_by_paragraphs(self, text: str) -> List[TextChunk]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º"""
        if not text.strip():
            return []
            
        paragraphs = self._split_by_paragraphs(text)
        if not paragraphs:
            return self.chunk_text_by_sentences(text)
        
        chunks = []
        current_chunk_sentences = []
        current_word_count = 0
        chunk_id = 0
        start_position = 0
        
        for paragraph in paragraphs:
            paragraph_sentences = self._split_by_sentences(paragraph)
            paragraph_word_count = self._get_word_count(paragraph)
            
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–∞–º –ø–æ —Å–µ–±–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
            if paragraph_word_count > self.max_chunk_size:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –µ—Å—Ç—å
                if current_chunk_sentences:
                    chunk = self._create_chunk_from_sentences(
                        current_chunk_sentences, chunk_id, start_position
                    )
                    chunks.append(chunk)
                    chunk_id += 1
                    start_position += len(' '.join(current_chunk_sentences))
                    current_chunk_sentences = []
                    current_word_count = 0
                
                # –†–∞–∑–¥–µ–ª—è–µ–º –±–æ–ª—å—à–æ–π –∞–±–∑–∞—Ü –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                para_chunks = self.chunk_text_by_sentences(paragraph)
                for para_chunk in para_chunks:
                    para_chunk.chunk_id = chunk_id
                    chunks.append(para_chunk)
                    chunk_id += 1
                
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –∞–±–∑–∞—Ü –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if (current_word_count + paragraph_word_count <= self.target_chunk_size or 
                not current_chunk_sentences):
                
                current_chunk_sentences.extend(paragraph_sentences)
                current_word_count += paragraph_word_count
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunk = self._create_chunk_from_sentences(
                    current_chunk_sentences, chunk_id, start_position
                )
                chunks.append(chunk)
                chunk_id += 1
                start_position += len(' '.join(current_chunk_sentences))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                overlap_sentences = current_chunk_sentences[-2:] if len(current_chunk_sentences) >= 2 else []
                current_chunk_sentences = overlap_sentences + paragraph_sentences
                current_word_count = self._get_word_count(' '.join(current_chunk_sentences))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk_sentences:
            chunk = self._create_chunk_from_sentences(
                current_chunk_sentences, chunk_id, start_position
            )
            chunks.append(chunk)
        
        return chunks
    
    def chunk_text_by_sentences(self, text: str) -> List[TextChunk]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
        if not text.strip():
            return []
            
        sentences = self._split_by_sentences(text)
        if not sentences:
            return []
        
        chunks = []
        current_sentences = []
        current_word_count = 0
        chunk_id = 0
        start_position = 0
        
        for sentence in sentences:
            sentence_word_count = self._get_word_count(sentence)
            
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–∞–º–æ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
            if sentence_word_count > self.max_chunk_size:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_sentences:
                    chunk = self._create_chunk_from_sentences(
                        current_sentences, chunk_id, start_position
                    )
                    chunks.append(chunk)
                    chunk_id += 1
                    start_position += len(' '.join(current_sentences))
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                chunk = self._create_chunk_from_sentences(
                    [sentence], chunk_id, start_position
                )
                chunks.append(chunk)
                chunk_id += 1
                start_position += len(sentence)
                current_sentences = []
                current_word_count = 0
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if (current_word_count + sentence_word_count <= self.target_chunk_size or 
                not current_sentences):
                
                current_sentences.append(sentence)
                current_word_count += sentence_word_count
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunk = self._create_chunk_from_sentences(
                    current_sentences, chunk_id, start_position
                )
                chunks.append(chunk)
                chunk_id += 1
                start_position += len(' '.join(current_sentences))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                overlap_sentences = current_sentences[-1:] if current_sentences else []
                current_sentences = overlap_sentences + [sentence]
                current_word_count = self._get_word_count(' '.join(current_sentences))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_sentences:
            chunk = self._create_chunk_from_sentences(
                current_sentences, chunk_id, start_position
            )
            chunks.append(chunk)
        
        return chunks
    
    def chunk_text(self, text: str, method: str = "paragraphs") -> List[TextChunk]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            method: –ú–µ—Ç–æ–¥ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è ("paragraphs" –∏–ª–∏ "sentences")
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        if not text or not text.strip():
            return []
        
        print(f"üî™ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (–º–µ—Ç–æ–¥: {method})")
        print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {self._get_word_count(text)} —Å–ª–æ–≤")
        print(f"üéØ –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {self.target_chunk_size} —Å–ª–æ–≤")
        
        if method == "paragraphs":
            chunks = self.chunk_text_by_paragraphs(text)
        else:
            chunks = self.chunk_text_by_sentences(text)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∞–Ω–∫–∞–º
        for i, chunk in enumerate(chunks):
            print(f"  üìÑ –ß–∞–Ω–∫ {i+1}: {chunk.word_count} —Å–ª–æ–≤, {chunk.sentence_count} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            if chunk.keywords:
                print(f"    üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(chunk.keywords[:5])}")
        
        return chunks
    
    def get_chunking_stats(self, chunks: List[TextChunk]) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∞–Ω–∫–∞–º"""
        if not chunks:
            return {"total_chunks": 0, "total_words": 0}
            
        total_words = sum(chunk.word_count for chunk in chunks)
        total_sentences = sum(chunk.sentence_count for chunk in chunks)
        avg_chunk_size = total_words / len(chunks) if chunks else 0
        
        return {
            "total_chunks": len(chunks),
            "total_words": total_words,
            "total_sentences": total_sentences,
            "average_chunk_size": round(avg_chunk_size, 1),
            "min_chunk_size": min(chunk.word_count for chunk in chunks),
            "max_chunk_size": max(chunk.word_count for chunk in chunks)
        }